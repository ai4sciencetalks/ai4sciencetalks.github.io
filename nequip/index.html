<!DOCTYPE html><html lang="en-gb"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>NequIP - AI4Science Talks</title><meta name="description" content="E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials Abstract: This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales. Speaker: Simon Batzner I am a fourth-year PhD student at Harvard University, fortunate to be advised by Boris Kozinsky. Prior to joining Harvard, I obtained a Master's from MIT, where I worked with Alexie Kolpak and Boris Kozinsky. At MIT, I also wrote a thesis on equivariant neural networks. Before that, I spent a year in Los Angeles, working on the NASA mission SOFIA, where I wrote software for analyzing telescope data and used ML to model the dynamics of piezolelectrics. I obtained my Bachelor's from the University of Stuttgart, Germany. I am originally from a small, but beautiful town a few minutes from the Bavarian Alps."><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/nequip/"><link rel="alternate" type="application/atom+xml" href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/feed.json"><meta property="og:title" content="NequIP"><meta property="og:image" content="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/media/posts/4/nequip-logo.png"><meta property="og:image:width" content="971"><meta property="og:image:height" content="537"><meta property="og:site_name" content="AI4Science Talks"><meta property="og:description" content="E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials Abstract: This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales. Speaker: Simon Batzner I am a fourth-year PhD student at Harvard University, fortunate to be advised by Boris Kozinsky. Prior to joining Harvard, I obtained a Master's from MIT, where I worked with Alexie Kolpak and Boris Kozinsky. At MIT, I also wrote a thesis on equivariant neural networks. Before that, I spent a year in Los Angeles, working on the NASA mission SOFIA, where I wrote software for analyzing telescope data and used ML to model the dynamics of piezolelectrics. I obtained my Bachelor's from the University of Stuttgart, Germany. I am originally from a small, but beautiful town a few minutes from the Bavarian Alps."><meta property="og:url" content="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/nequip/"><meta property="og:type" content="article"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@ai4sciencetalks"><meta name="twitter:title" content="NequIP"><meta name="twitter:description" content="E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials Abstract: This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales. Speaker: Simon Batzner I am a fourth-year PhD student at Harvard University, fortunate to be advised by Boris Kozinsky. Prior to joining Harvard, I obtained a Master's from MIT, where I worked with Alexie Kolpak and Boris Kozinsky. At MIT, I also wrote a thesis on equivariant neural networks. Before that, I spent a year in Los Angeles, working on the NASA mission SOFIA, where I wrote software for analyzing telescope data and used ML to model the dynamics of piezolelectrics. I obtained my Bachelor's from the University of Stuttgart, Germany. I am originally from a small, but beautiful town a few minutes from the Bavarian Alps."><meta name="twitter:image" content="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/media/posts/4/nequip-logo.png"><link rel="stylesheet" href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/assets/css/style.css?v=c3b4ba00367efc548a8cd2c93a953db2"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/nequip/"},"headline":"NequIP","datePublished":"2022-12-22T21:39","dateModified":"2022-12-22T22:02","image":{"@type":"ImageObject","url":"https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/media/posts/4/nequip-logo.png","height":537,"width":971},"description":"E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials Abstract: This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales. Speaker: Simon Batzner I am a fourth-year PhD student at Harvard University, fortunate to be advised by Boris Kozinsky. Prior to joining Harvard, I obtained a Master's from MIT, where I worked with Alexie Kolpak and Boris Kozinsky. At MIT, I also wrote a thesis on equivariant neural networks. Before that, I spent a year in Los Angeles, working on the NASA mission SOFIA, where I wrote software for analyzing telescope data and used ML to model the dynamics of piezolelectrics. I obtained my Bachelor's from the University of Stuttgart, Germany. I am originally from a small, but beautiful town a few minutes from the Bavarian Alps.","author":{"@type":"Person","name":"Marimuthu Kalimuthu","url":"https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/authors/kmario23/"},"publisher":{"@type":"Organization","name":"Marimuthu Kalimuthu"}}</script></head><body class="lines"><div class="container lines lines--right"><header class="header"><a href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/" class="logo">AI4Science Talks</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu">Menu</button><ul class="navbar__menu"><li><a href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/" target="_self">Home</a></li><li><a href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/talks/" target="_self">Talks</a></li></ul></nav></header><main class="main"><article class="post"><header class="post__inner post__header"><h1 class="post__title">NequIP</h1><div class="post__meta"><div class="post__meta__left"><a href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/authors/kmario23/" class="invert post__author" rel="author" title="Marimuthu Kalimuthu">Marimuthu Kalimuthu</a></div><div class="post__meta__right"><time datetime="2022-12-22T21:39" class="post__date">December 22, 2022</time><div class="post__updated">Updated on <time datetime="2022-12-22T21:39" class="post__date">December 22, 2022</time></div></div></div></header><figure class="post__featured-image"><div class="post__featured-image__inner is-img-loading"><img src="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/media/posts/4/nequip-logo.png" srcset="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/media/posts/4/responsive/nequip-logo-xs.png 384w, https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/media/posts/4/responsive/nequip-logo-sm.png 600w, https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/media/posts/4/responsive/nequip-logo-md.png 768w, https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/media/posts/4/responsive/nequip-logo-lg.png 1200w, https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/media/posts/4/responsive/nequip-logo-xl.png 1600w" sizes="(min-width: 37.5em) 1600px, 80vw" loading="eager" height="537" width="971" alt=""></div></figure><div class="post__inner"><div class="post__entry"><h6><strong><a href="https://arxiv.org/abs/2101.03164" target="_blank" rel="noopener noreferrer">E(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials</a></strong></h6><hr><p><em>Abstract</em>:</p><p>This work presents Neural Equivariant Interatomic Potentials (NequIP), an E(3)-equivariant neural network approach for learning interatomic potentials from ab-initio calculations for molecular dynamics simulations. While most contemporary symmetry-aware models use invariant convolutions and only act on scalars, NequIP employs E(3)-equivariant convolutions for interactions of geometric tensors, resulting in a more information-rich and faithful representation of atomic environments. The method achieves state-of-the-art accuracy on a challenging and diverse set of molecules and materials while exhibiting remarkable data efficiency. NequIP outperforms existing models with up to three orders of magnitude fewer training data, challenging the widely held belief that deep neural networks require massive training sets. The high data efficiency of the method allows for the construction of accurate potentials using high-order quantum chemical level of theory as reference and enables high-fidelity molecular dynamics simulations over long time scales.</p><hr><p><em>Speaker</em>: Simon Batzner</p><p>I am a fourth-year PhD student at Harvard University, fortunate to be advised by <a href="https://bkoz.seas.harvard.edu/">Boris Kozinsky</a>. Prior to joining Harvard, I obtained a Master's from MIT, where I worked with Alexie Kolpak and Boris Kozinsky. At MIT, I also wrote a thesis on equivariant neural networks. Before that, I spent a year in Los Angeles, working on the NASA mission <a href="https://www.nasa.gov/mission_pages/SOFIA/index.html">SOFIA</a>, where I wrote software for analyzing telescope data and used ML to model the dynamics of piezolelectrics. I obtained my Bachelor's from the University of Stuttgart, Germany. I am originally from a <a href="https://en.wikipedia.org/wiki/Illertissen">small, but beautiful town a few minutes from the Bavarian Alps</a>.</p></div><footer class="post__footer"><div class="post__tag-share"><div class="post__tag"><h3 class="post__tag__title">Posted in</h3><ul class="post__tag__list"><li><a href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/tags/all/">All</a></li><li><a href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/tags/md-simulations/">md-simulations</a></li></ul></div><div class="post__share"><a href="https://twitter.com/share?url=https%3A%2F%2Fai4sciencetalks.github.io%2Fai4sciencetalks.github.io%2Fnequip%2F&amp;via=ai4sciencetalks&amp;text=NequIP" class="js-share twitter" aria-label="Share with Twitter" aria-label="Share with Twitter" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/assets/svg/svg-map.svg#twitter"/></svg> </a><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fai4sciencetalks.github.io%2Fai4sciencetalks.github.io%2Fnequip%2F" class="js-share linkedin" aria-label="Share with LinkedIn" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/assets/svg/svg-map.svg#linkedin"/></svg></a></div></div></footer></div></article></main><footer class="footer"><div class="footer__left"><ul class="footer__nav"></ul><div class="footer__copy">Created with ❤️</div></div><div class="footer__right"><div class="footer__social"><a href="https://twitter.com/ai4sciencetalks" aria-label="Twitter" class="twitter"><svg><use xlink:href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/assets/svg/svg-map.svg#twitter"/></svg> </a><a href="https://www.youtube.com/@ai4sciencetalks" aria-label="Youtube" class="youtube"><svg><use xlink:href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/assets/svg/svg-map.svg#youtube"/></svg></a></div></div><button onclick="backToTopFunction()" id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg><use xlink:href="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = { mobileMenuMode: 'sidebar', animationSpeed: 300, submenuWidth: 'auto', doubleClickTime: 500, mobileMenuExpandableSubmenus: true, relatedContainerForOverlayMenuSelector: '.navbar', };</script><script defer="defer" src="https://ai4sciencetalks.github.io/ai4sciencetalks.github.io/assets/js/scripts.min.js?v=8190bbfcf662a6d5bf1ad35d88ad1ec9"></script><script>function publiiDetectLoadedImages () {
         var images = document.querySelectorAll('img[loading]:not(.is-loaded)');
         for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
               images[i].classList.add('is-loaded');
               images[i].parentNode.classList.remove('is-img-loading');
            } else {
               images[i].addEventListener('load', function () {
                  this.classList.add('is-loaded');
                  this.parentNode.classList.remove('is-img-loading');
               }, false);
            }
         }
      }
      publiiDetectLoadedImages();</script></body></html>